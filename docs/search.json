[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "rogeriog - Rogério Gouvêa",
    "section": "",
    "text": "Hello, I’m Rogério, a soon-to-be PhD in Materials Science. My academic work has been focused on using physics and machine learning to discover insights in energy materials.\nI’m enthusiastic about programming, data science, and AI, and I’m dedicated to improving productivity while exploring fresh horizons in my academic work and development projects.\nThis website blends my work portfolio with a digital garden, housing information on past projects, ongoing work, and my learning journey. Explore my digital world &lt;ᵠ|here|ᵠ&gt;."
  },
  {
    "objectID": "index.html#a-computational-materials-scientist-always-programming-some-stuff",
    "href": "index.html#a-computational-materials-scientist-always-programming-some-stuff",
    "title": "rogeriog - Rogério Gouvêa",
    "section": "",
    "text": "Hello, I’m Rogério, a soon-to-be PhD in Materials Science. My academic work has been focused on using physics and machine learning to discover insights in energy materials.\nI’m enthusiastic about programming, data science, and AI, and I’m dedicated to improving productivity while exploring fresh horizons in my academic work and development projects.\nThis website blends my work portfolio with a digital garden, housing information on past projects, ongoing work, and my learning journey. Explore my digital world &lt;ᵠ|here|ᵠ&gt;."
  },
  {
    "objectID": "academic/MODnet.html",
    "href": "academic/MODnet.html",
    "title": "MODNet",
    "section": "",
    "text": "All-round framework? Feed forward neural network? info goes one layer after the other\nWhat does selection of meaningful features mean? Joint-learning.\nWhat are graph network models? Joint learning vs single target learning. – it seems joint learning predicts multiple properties at once, and it improves the prediction with similar properties probably.\nRead these reviews of ML in materials, butler Schmidt e noh\nDifferent feature generation used in materials ML?\nUnderlying ML models: - Ad hoc models, case by case, specific group of materials, specific properties. Tailored descriptors, see references 5678, simpler to work case by case, better accuracy, poor transfer. - More general models based on graph network , raw crystal becomes a graph and then deep layer, ref 9 10 11. - -\nPymatgen to query - MaterialsProject GCNN –&gt; MEGNet (needs to know full structure) MODNet is flexible can use structure, but also just composition. Matminer gives basic input data to MODNet model. Modnet implements featurization. receives pymatgen composition,\nRead\nGet a dataset (Matminer ) Test different models (basic models –&gt; MEGNet – MODNet) Compare scores, cross validation - validation and test sets (MatBench)\n\nMatminer + Scikit\n\n\nMatBench\n\n\n\n\n Back to top"
  },
  {
    "objectID": "academic/Encoder - Neural Network Compression.html",
    "href": "academic/Encoder - Neural Network Compression.html",
    "title": "Encoder - Neural Network Compression",
    "section": "",
    "text": "This tutorial has a full implementation https://machinelearningmastery.com/autoencoder-for-regression/\nThere is also one for classification: https://machinelearningmastery.com/autoencoder-for-classification/\nI think this text has important information regarding the representation in the autoencoder, why compression may explain better results in ML models: https://neptune.ai/blog/representation-learning-with-autoencoder\n\n\n\n Back to top"
  },
  {
    "objectID": "academic/index.html",
    "href": "academic/index.html",
    "title": "Academic Work - Research",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nEncoder - Neural Network Compression\n\n\n1 min\n\n\n\nMachine Learning\n\n\nEncoders\n\n\n\n\n\n\n\nDec 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMODNet\n\n\n1 min\n\n\n\nMachine Learning\n\n\nMODNet\n\n\n\n\n\n\n\nDec 17, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSOAP featurization\n\n\n2 min\n\n\n\nMachine Learning\n\n\nFeaturization\n\n\nSOAP\n\n\n\n\n\n\n\nDec 17, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "academic/SOAP featurization.html",
    "href": "academic/SOAP featurization.html",
    "title": "SOAP featurization",
    "section": "",
    "text": "28:57 SOAP.\n\n\nHer videos are very important to undersatnad featurization.\nI think i should try to use the soap in similar structures and see if I can detect similar motifs in different structures using distance, and varying cutoff. Then generalize soap for all elements and test a bunch of them. With the flattened soap, I can use DBSCAN to group similar structures. In the end i will have for the OFM matrix, a list of clusterings from 1 to maybe 70 types of clustering associated with each structure. This will be transformed in a hotencode, 1 to 70 columns with 1 in the right correspondent. This will be appended to the featurized dataframe and will be predicted by megnet, this way megnet will be predicting the chemical subgroup detected with the OFM matrix. The same will be done for the SOAP featurizer, but in this case we will vary the cutoff and have probably SOAP enconding for 2 Angs, 4 Angs and 6 Angs, each one with all its corresponding cluster encodings. It is important to preserve chemical interpretability to find the center structure of each clustering that can be traced back to the corresponding chemical environment.\nMaybe just a descriptor of every site and check each site including it as a feature.\nTo install SOAP\n conda install ase\n conda install dscribe\n\n\n\n Back to top"
  }
]